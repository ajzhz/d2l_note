{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72269ba6",
   "metadata": {},
   "source": [
    "# 优化算法\n",
    "\n",
    "优化问题<br>\n",
    "* 一般形式\n",
    "$$minimize \\ f(x)  \\quad subject \\ to \\ x \\in C$$\n",
    "f就是损失函数<br>\n",
    "\n",
    "## 局部最小vs全局最小\n",
    "* 全局最小$\\mathbf (x^*): f(x^*) \\leq f(x) \\quad \\forall x \\in C$\n",
    "* 局部最小$\\mathbf (x^*): $ 存在$\\epsilon$, 使得 $f(x^*) \\leq f(x) \\quad \\forall x: ||x - x^*|| \\leq \\epsilon$\n",
    "\n",
    "<img src='./image/minimize.jpg' alt='minimize' width=400><br>\n",
    "\n",
    "## 凸\n",
    "### 凸集\n",
    "任意找两个点随即连一根线这个线也在集合中\n",
    "* 一个$\\mathbb R^n$的子集C是凸当且仅当\n",
    "$$\n",
    "\\alpha x + (1 - \\alpha)y \\in C \\\\\n",
    "\\forall \\alpha \\in [0,1] \\ \\forall x,y \\in C\n",
    "$$\n",
    "<img src='./image/convexSet.jpg' alt='convexSet' width=200><br>\n",
    "\n",
    "### 凸函数\n",
    "随机取两个点，整个函数都在该线下面\n",
    "* 函数$f: C \\rightarrow \\mathbb R$是凸当且仅当\n",
    "$$\n",
    "f(\\alpha x + (1 - \\alpha )y \\leq \\alpha f(x) + (1- \\alpha )f(y)) \\\\\n",
    "\\forall \\alpha \\in [0,1] \\ \\forall x,y \\in C\n",
    "$$\n",
    "<img src='./image/convexFunc.jpg' alt='convexFunc' width=300><br>\n",
    "\n",
    "* 如果$x \\not= y, \\alpha \\in (0,1)$时不等式严格成立，那么叫严格凸函数\n",
    "\n",
    "### 凸函数优化\n",
    "* 如果代价函数$f$时凸的，切限制集合$C$是凸的，那么就是凸优化问题，那么局部最小一定是全局最小\n",
    "* 严格凸优化问题有唯一的全局最小\n",
    "\n",
    "<img src='./image/convexOpt.jpg' alt='convexOpt' width=300><br>\n",
    "\n",
    "\n",
    "### 凸和非凸例子\n",
    "* 凸\n",
    "  * 线性回归 $f(x) = ||Wx-b||^2_2$\n",
    "  * softmax回归\n",
    "* 非凸\n",
    "  * MLP, CNN, RNN, attention\n",
    "\n",
    "<img src='./image/example.jpg' alt='example' width=200><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e399e5a",
   "metadata": {},
   "source": [
    "## 梯度下降\n",
    "* 最简单的迭代求解短发\n",
    "* 选取开始点$x_0$\n",
    "* 对$t = 1,...,T$\n",
    "  * $x_t = x_{t-1} - \\eta \\nabla f(x_{t-1})$\n",
    "* $\\eta$ 叫做学习率\n",
    "\n",
    "<img src='./image/gradientDescent.jpg' alt='gradientDescent' width=200><br>\n",
    "\n",
    "### 随机梯度下降\n",
    "* 有n个样本时，计算$f(x) = \\frac 1 n \\sum ^n_{i=0} \\ell _i(x) $的导数太贵\n",
    "* 随机梯度下降在时间t随机选项样本$t_i$来近似f(x)\n",
    "$$\n",
    "x_t = x_{t-1} - \\eta \\nabla \\ell (x_{t-1}) \\\\\n",
    "\\mathbb E \\left [ \\nabla \\ell _{t_i}(x) \\right] = \\mathbb E [\\nabla f(x)]\n",
    " $$\n",
    "<img src='./image/SGD.jpg' alt='SGD' width=200><br>\n",
    "\n",
    "### 小批量梯度下降\n",
    "* 计算单样本的梯度完全利用硬件资源\n",
    "* 小批量随机梯度下降在时间t采样一个随机子集$I_t \\subset \\{ 1,...,n \\}$ 使得$|I_t|=b$\n",
    "\n",
    "* 同样，这是个无偏的近似但是降低了方差\n",
    "$$\\mathbb E  [ \\frac 1 b \\sum_{i \\in I_i} \\nabla \\ell(x)  ] = \\nabla f(x)$$\n",
    "期望不变，而且对比单个样本的随机梯度，方差减小了<br>\n",
    "<img src='./image/BSGD.jpg' alt='BSGD' width=300><br>\n",
    "\n",
    "### 冲量法\n",
    "维护一个冲量，保证平滑的改变方向，使用$v_t$来计算梯度\n",
    "* 冲量法使用平滑过的梯度对权重更新\n",
    "$$\n",
    "g_t = \\frac 1 b \\sum_{i \\in I_i} \\nabla \\ell(x_{t-1}) \\\\\n",
    "v_t = \\beta v_{t-1} + g_t \\quad w_t = w_{t-1} - \\eta v_t\n",
    "$$\n",
    "* $\\beta$常见取值[0.5, 0.9, 0.95, 0.99]\n",
    "\n",
    "所以可以看到过去很多个梯度的值，并每次将低一点权重<br>\n",
    "<img src='./image/imm.jpg' alt='imm' width=200><br>\n",
    "momentum选项就是冲量法\n",
    "\n",
    "### adam\n",
    "非常平滑，对学习率没有那么敏感\n",
    "* 记录$v_t = \\beta _1 v_{t-1} + (1-\\beta_1)g_t$\n",
    "* 展开$v_t = (1-\\beta_1)(g_t + \\beta_1g_{t-1} + \\beta_2g_{t-2}^2 + \\beta_3g_{t-3}^3 + ...)$\n",
    "* 因为$\\sum ^{\\inf}_{i=0} \\beta^i_1 = \\frac 1 {1-\\beta_1} $, 所以权重和为1，数列收敛到\n",
    "* 由于$v_0 = 0$，且$\\sum ^t_{i=0} \\beta ^i_1 = \\frac {1- \\beta^t_1}{1- \\beta_1}$ \n",
    "* 因此需要修正(仅对t小的时候有用): $\\hat v_t = \\frac {v_t}{1-\\beta^t_1}$\n",
    "* 类似记录$s_t = \\beta_2 s_{t-1} + (1-\\beta_2) g_t^2$，通常$\\beta_2 = 0.999, $且修正$\\hat s_t = \\frac {s_t}{1-\\beta_2^t}$\n",
    "* 计算重新调整后的梯度$g'_t = \\frac {\\hat v_t}{\\sqrt {\\hat s_t} + \\epsilon}$ 按元素除\n",
    "* 最后更新$w_t = w_{t-1} - \\eta g'_t$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8fd1b5",
   "metadata": {},
   "source": [
    "# The end\n",
    "[结合代码了解细节](https://paperswithcode.com/)<br>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
