{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积\n",
    "两个原则：平移不变性，局部性<br>\n",
    "## 重新考察全连接层\n",
    "需要将权重变为4-D张量: $$h_{i,j} = \\sum _{k,l} w_{i,j,k,l} x_{k,l} = \\sum _{a,b} v_{i,j,a,b} x_{i+a,j+b}$$\n",
    "V是W的重新索引 $v_{i,j,a,b} = w_{i,j,i+a,j+b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原则1 平移不变性\n",
    "希望做到x平移后，h不应当平移\n",
    "因为在式 $$h_{i,j} = \\sum _{a,b} v_{i,j,a,b} x_{i+a,j+b}$$\n",
    "中，x平移会导致h的平移因此解决方案是：$v_{i,j,a,b} = v_{a,b}$ <br>\n",
    "$$h_{i,j} = \\sum _{a,b} v_{a,b} x_{i+a,j+b}$$\n",
    "这就是二维卷积（交叉相关）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原则2 局部性\n",
    "$$h_{i,j} = \\sum _{a,b} v_{a,b} x_{i+a,j+b}$$\n",
    "解决方案: $$ h_{i,j} = \\sum _{a = - \\Delta}^\\Delta \\sum _{b = - \\Delta}^\\Delta v_{a,b} x_{i+a,j+b}$$\n",
    "这便是将全连接层转变为了卷积层的过程\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二维交叉相关（对应项相乘后相加）<br>\n",
    "输入X：$n_h * n_w$<br>\n",
    "核W：$k_h * k_w $<br>\n",
    "偏差b<br>\n",
    "输出Y：$(n_h - k_h + 1) * (n_w - k_w + 1)$<br>\n",
    "$$Y = X*W +b$$\n",
    "W和b都是可学习的参数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 互相关运算实现（卷积）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def corr2d(X, K):  # X输入，K核矩阵\n",
    "    \"\"\"计算二维互相关运算。\"\"\"\n",
    "    h, w = K.shape # 行数 列数\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum() # * .sum()实现点乘运算\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现二维卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size)) # 将这个卷积核设置为可训练的参数\n",
    "        self.bias = nn.Parameter(torch.zeros(1)) # 广播机制或许可以将这个偏移置于全局\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积层应用：检测图像边缘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "X # 白变黑黑变白的两根线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = torch.tensor([[1.0, -1.0]]) # 边缘检测\n",
    "K.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该卷积核只能检测竖向的边"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X.t(), K) # 转置后的矩阵无法检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2, loss 12.647\n",
      "batch 4, loss 2.645\n",
      "batch 6, loss 0.658\n",
      "batch 8, loss 0.198\n",
      "batch 10, loss 0.069\n"
     ]
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias=False)\n",
    "\n",
    "X = X.reshape((1, 1, 6, 8)) # 批量大小数 通道数 图片的两个维度\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X) # 设置预测值（正向传播）\n",
    "    l = (Y_hat - Y)**2 # 损失函数\n",
    "    conv2d.zero_grad() # 梯度清零\n",
    "    l.sum().backward() # 计算梯度 计算得到的梯度会被存储至对应张量的.grad中\n",
    "    conv2d.weight.data[:] -= 3e-2 * conv2d.weight.grad # 梯度下降 学习率*梯度\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'batch {i+1}, loss {l.sum():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所学习的卷积层权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0087, -0.9577]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data.reshape((1, 2)) # 结果接近 1.-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解决抖动：1.增大批量大小 2.增大学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 填充和步幅\n",
    "### 填充\n",
    "在输入周围添加额外的行和列<br>\n",
    "填充$p_h$行和$p_w$列，输出形状为$$(n_h - k_h + p_h + 1) * (n_w - k_w +p_w + 1)$$\n",
    "通常取$p_h = k_h - 1, p_w = k_w - 1$这样输入输出大小不会发生变化<br>\n",
    "$k_h$为偶数时填充$p_h / 2$\n",
    "### 步幅\n",
    "控制每次不止移动一格\n",
    "给定高度$s_h$和宽度$s_w$的步幅，输出形状是$$(n_h - k_h + p_h + s_h) / s_h * (n_w - k_w +p_w + s_w) / s_w$$\n",
    "假设$p_h = k_h - 1, p_w = k_w - 1$\n",
    "$$(n_h + s_h - 1) / s_h * (n_w + s_w - 1) / s_w$$\n",
    "假设输入高度和宽度可以被步幅整除\n",
    "$$(n_h / s_h) * (n_w / s_w)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def comp_conv2d(conv2d, X):\n",
    "    X = X.reshape((1, 1) + X.shape) # 在维度前加入批量大小和通道数\n",
    "    Y = conv2d(X)\n",
    "    return Y.reshape(Y.shape[2:]) # 只返回最后两个维度即矩阵大小，前两个维度是批量大小和通道数\n",
    "\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1) # padding =1上下左右都填充一行\n",
    "X = torch.rand(8, 8)\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1)) # padding = (2,1) 上下填充两行，左右填充一列\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)\n",
    "comp_conv2d(conv2d, X).shape # 根据公式计算出形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4)) \n",
    "comp_conv2d(conv2d, X).shape # 根据公式计算出形状,向下取整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多个输入和输出通道\n",
    "### 多个输入通道\n",
    "彩色图片有RGB三个通道<br>\n",
    "每个通道都有一个卷积核，结果是所有通道的卷积结果的和<br>\n",
    "单输出通道<br>\n",
    "输入X: $c_i \\times n_h \\times n_w$<br>\n",
    "核W:$ c_i \\times k_h \\times k_w$<br>\n",
    "输出Y: $ m_h \\times m_w$<br>\n",
    "$$Y = \\sum _{i=0}^{c_i} X_{i,:,:} \\star W_{i,:,:}$$\n",
    "其中$\\star$意为卷积操作\n",
    "\n",
    "### 多输出通道\n",
    "输入X: $c_i \\times n_h \\times n_w$<br>\n",
    "核W:$ c_o \\times c_i \\times k_h \\times k_w$<br>\n",
    "输出Y: $ c_o \\times m_h \\times m_w$<br>\n",
    "$$Y_{i,:,:} = X \\star W_{i,:,:,:} \\quad for \\: i=1,...,c_o$$\n",
    "每个输出通道可以识别为特定模式\n",
    "\n",
    "### 1 * 1卷积层\n",
    "不识别空间模式，仅仅融合通道，相当于输入形状为$n_h n_w \\times c_i$ 经过一个形状为$c_o \\times c_i$ 输出为$n_h n_w \\times c_o$\n",
    "\n",
    "### 二维卷积层\n",
    "输入X: $c_i \\times n_h \\times n_w$<br>\n",
    "核W: $c_o \\times c_i \\times k_h \\times k_w$ 共有$c_o \\times c_i$个核<br>\n",
    "偏差B: $c_o \\times c_i$ 每个核一个<br>\n",
    "输出Y: $ c_o \\times m_h \\times m_w$<br>\n",
    "$$Y = X \\star W + B$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 3]),\n",
       " torch.Size([2, 2, 2]),\n",
       " torch.Size([2, 2]),\n",
       " tensor([[ 56.,  72.],\n",
       "         [104., 120.]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def corr2d_multi_in(X, K): # 多输入通道\\\n",
    "    return sum(corr2d(x, k) for x, k in zip(X, K)) # zip将两个列表打包成一个元组列表\n",
    "# 实际是拿出每个通道的x,k分别计算后相加\n",
    "# output = torch.sum(torch.stack([torch.conv2d(X[i], K[i]) for i in range(C_in)]), dim=0)\n",
    "# 该代码相当于，先在stack中拼接后再在第一维度即新增的维度相加，因为torch.sum只能对张量运算\n",
    "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
    "                  [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "\n",
    "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
    "\n",
    "X.shape, K.shape, corr2d_multi_in(X, K).shape ,corr2d_multi_in(X, K)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3]) torch.Size([2, 2])\n",
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.],\n",
      "        [6., 7., 8.]]) tensor([[0., 1.],\n",
      "        [2., 3.]])\n",
      "torch.Size([3, 3]) torch.Size([2, 2])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]]) tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "for x,k in zip(X, K):\n",
    "    print(x.shape, k.shape)\n",
    "    print(x, k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算多个通道的输出的互相关函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def corr2d_multi_in_out(X, K): # 多输入多输出通道 \n",
    "    return torch.stack([corr2d_multi_in(X, k) for k in K]) # 对每个k进行卷积\n",
    "\n",
    "# 每个 for都是一次降维的过程\n",
    "K = torch.stack((K, K + 1, K + 2))\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证1*1 卷积等价于全连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def corr2d_multi_in_out_1x1(X, K):\n",
    "    c_i, h, w = X.shape\n",
    "    c_o = K.shape[0]\n",
    "    X = X.reshape((c_i, h * w))\n",
    "    K = K.reshape((c_o, c_i))\n",
    "    Y = torch.matmul(K, X)\n",
    "    return Y.reshape((c_o, h, w))\n",
    "\n",
    "X = torch.normal(0, 1, (3, 3, 3))\n",
    "K = torch.normal(0, 1, (2, 3, 1, 1))\n",
    "\n",
    "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
    "Y2 = corr2d_multi_in_out(X, K)\n",
    "assert float(torch.abs(Y1 - Y2).sum()) < 1e-6\n",
    "Y1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 池化层\n",
    "卷积层对位置非常的敏感，因此我们希望将他更好的泛化，当图片发生一定程度的变化时，输出不要发生太大的变化，类似需要一点平移不变性<br>\n",
    "\n",
    "### 二维最大池化\n",
    "返回滑动窗口的最大值（卷积窗口的最大值）<br>\n",
    "池化层与卷积层类似，都具有填充和步幅<br>\n",
    "没有可学习的参数<br>\n",
    "在每个输入通道应用池化层获得相应的输出通道<br>\n",
    "输出通道数 = 输出通道数 （注意是通道数，不是形状）<br>\n",
    "\n",
    "### 平均池化层\n",
    "平均的池化\n",
    "\n",
    "### 总结\n",
    "池化层返回窗口中的最大或平均值<br>\n",
    "缓解卷积层会位置敏感性<br>\n",
    "同样有窗口大小、填充和步幅作为参数<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现池化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4., 5.],\n",
       "         [7., 8.]]),\n",
       " tensor([[2., 3.],\n",
       "         [5., 6.]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def pool2d(X, pool_size, mode='max'): # 池化函数 mode就是最大还是平均\n",
    "    p_h, p_w = pool_size\n",
    "    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i:i + p_h, j:j + p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i:i + p_h, j:j + p_w].mean()\n",
    "    return Y\n",
    "\n",
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "pool2d(X, (2, 2)),pool2d(X, (2, 2), 'avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 填充和步幅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[10.]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3) # pytorch框架中似乎规定了窗口不重叠\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "填充和步幅可以手动设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  3.],\n",
       "          [ 9., 11.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d((2, 3), padding=(1, 1), stride=(2, 3))\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "池化层在每个输入通道上单独运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]],\n",
       "\n",
       "         [[ 1.,  2.,  3.,  4.],\n",
       "          [ 5.,  6.,  7.,  8.],\n",
       "          [ 9., 10., 11., 12.],\n",
       "          [13., 14., 15., 16.]]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.cat((X, X + 1), 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]],\n",
       "\n",
       "         [[ 6.,  8.],\n",
       "          [14., 16.]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
